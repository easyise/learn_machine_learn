{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7847cac8",
   "metadata": {},
   "source": [
    "# Учись Учить Машину / Learn Machine Learning\n",
    "\n",
    "<img src=\"data/lml.png\" width=200>\n",
    "\n",
    "Онлайн-лекции Ильи С. Елисеева: применение методов машинного обучения в анализе данных.\n",
    "\n",
    "- Канал в Telegram: https://t.me/learn_machine_learn\n",
    "- YouTube: https://www.youtube.com/channel/UCCwDwKatNitBCVAJajremMQ\n",
    "- VK: https://vk.com/learn_machine_learn\n",
    "- GitHub: https://github.com/easyise/learn_machine_learn\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4629c6eb",
   "metadata": {},
   "source": [
    "# Лекция 1. Нестандартные распределения и статистические выбросы"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d45f697",
   "metadata": {},
   "source": [
    "## Немного теории: виды распределений вероятностей\n",
    "\n",
    "При работе с количественными непрерывными и дискретными признаками вам возможно придется иметь дело со следующими видами распределений:\n",
    "- __Равномерное__: \n",
    "    - любое значение в диапазоне равновероятно (потому и оно равномерное), \n",
    "    - нет ярко выраженной моды, все значения лежат в некотором диапазоне, \n",
    "    - гистограмма близка к \"плоской\", \n",
    "    - ...встречается редко, но если вам повезло и на гитограмме \"плоско\" - это признак машинногенерированных или последовательных данных, шума или намеренной обфускации исходных данных.\n",
    "\n",
    "- __Нормальное (Гауссово)__ - идеальный случай:\n",
    "    - есть ярко выраженая мода (вершина \"колокола Гаусса\"), \n",
    "    - медиана приближена к среднему,\n",
    "    - соответствует ЦПТ, ЗБЧ и другим \"сферическим коням в вакууме\" из мира статистики\n",
    "\n",
    "    ПРИМЕРЫ: количество минут в телефонном разговоре, пробег грузовика за месяц, количество проданного хлеба за сутки и т.д.\n",
    "\n",
    "\n",
    "- __Логнормальное__ - распределение, которое приводится к нормальному после логарифмирования величин. Характерно для следующих данных:\n",
    "    - доходы физических и юридических лиц\n",
    "    - количество комментариев под постами в соц. сетях и интернет-магазинах\n",
    "    - другое...\n",
    "\n",
    "\n",
    "\n",
    "- __Случайное__: \n",
    "    - есть одна или несколько ярко выраженных мод, \n",
    "    - есть выбросы, \n",
    "    - данные лежат в широчайшем диапазоне, \n",
    "    - ...в общем, все как мы любим.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb5c3a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224699e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# генерируем синтетические данные\n",
    "size = 10000\n",
    "\n",
    "data_unif = np.random.uniform(size=size)\n",
    "data_norm = np.random.normal(size=size)\n",
    "data_lognorm = np.random.lognormal(size=size)\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots( 1, 3 )\n",
    "fig.set_size_inches( (15, 5) )\n",
    "axs[0].set_title('Равномерное')\n",
    "axs[1].set_title('Нормальное (Гауссово)')\n",
    "axs[2].set_title('Логнормальное')\n",
    "\n",
    "sns.histplot(data_unif, ax=axs[0], kde=True)\n",
    "sns.histplot(data_norm, ax=axs[1], kde=True)\n",
    "sns.histplot(data_lognorm, ax=axs[2], kde=True)\n",
    "\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b112172d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# загрузим реальные данные:\n",
    "# отзывы на локальные бизнесы из Yelp\n",
    "df_reviews = pd.read_csv('data/reviews.csv', index_col=0)\n",
    "display(df_reviews)\n",
    "\n",
    "# отток клиентов в телекоммуникационной компании\n",
    "df_telecom = pd.read_csv('data/telecom_churn.csv')\n",
    "display(df_telecom)\n",
    "\n",
    "# кредитный скоринг\n",
    "df_credit = pd.read_csv('data/credit_scoring.csv', index_col=0)\n",
    "df_credit['Income'] = df_credit['Income'].fillna(df_credit['Income'].median())\n",
    "display(df_credit)\n",
    "\n",
    "# модели журнала Playboy\n",
    "df_playboy = pd.read_csv('data/girls.csv')\n",
    "display(df_playboy)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91efd963",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots( 3, 2 )\n",
    "fig.set_size_inches( (10, 12) )\n",
    "axs = axs.ravel()\n",
    "axs[0].set_title('Количество отзывов на бизнесы в Yelp')\n",
    "axs[1].set_title('Количество голосовых сообщений')\n",
    "axs[2].set_title('Кредитный скоринг: Income')\n",
    "axs[3].set_title('Талия у Playmates')\n",
    "axs[4].set_title('Год выхода журнала Playboy')\n",
    "\n",
    "sns.histplot(df_reviews['review_count'], ax=axs[0])\n",
    "sns.histplot(df_telecom['Number vmail messages'], ax=axs[1])\n",
    "sns.histplot(df_credit['Income'], ax=axs[2])\n",
    "sns.histplot(df_playboy['Waist'], ax=axs[3])\n",
    "sns.histplot(df_playboy['Year'], ax=axs[4])\n",
    "\n",
    "axs[5].axis('off')\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e12fc010",
   "metadata": {},
   "source": [
    "Чем плохи некоторые из этих распределений?\n",
    "- они __ассиметричны__, имеют длинный хвост справа\n",
    "- данные имеют __широкий диапазон__ значений: от 0 до 1e3 - 1e6\n",
    "- в них много __выбросов__\n",
    "- они __не нормальны__, что нарушает предпосылки многих моделей машинного обучения.\n",
    "\n",
    "Самое главное - для использовании линейных моделей и кластеризации данные **не масштабированы** и потому непригодны."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8b8fa19",
   "metadata": {},
   "source": [
    "**Что делать?** Можно ли привести данные к нормальному или как можно близкому к нормальному распределению?\n",
    "\n",
    "Вот какие способы можно для этого использовать:\n",
    "- избавление от выбросов\n",
    "- нормализация или шкалирование данных\n",
    "- квантильные преобазования или бининг, категоризация данных\n",
    "- степенные преобразования: логарифмирование, преобразования Бокса-Кокса или Йео-Джонсона\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dad754a8",
   "metadata": {},
   "source": [
    "## 1. Обнаружение выбросов и избавление от них\n",
    "\n",
    "По классике: измерим межквартильный размах - различие между величиной 3-го квартиля и 1-го квартиля. \n",
    "**Межквантильный размах** (англ. *interquartile range*, **IQR**) — это статистическая мера **разброса данных**, основанная на **квартилях**. \n",
    "\n",
    "$\n",
    "\\text{IQR} = Q_3 - Q_1\n",
    "$\n",
    "\n",
    "* $Q_1$ — первый квартиль (25‑й процентиль)\n",
    "* $Q_3$ — третий квартиль (75‑й процентиль)\n",
    "\n",
    "\n",
    "\n",
    "#### **Поиск выбросов** (outliers):\n",
    "\n",
    "Классическое правило Тьюки (Tukey):\n",
    "\n",
    "* **Нижняя граница**:\n",
    "\n",
    "  $\n",
    "  Q_1 - 1.5 \\cdot IQR\n",
    "  $\n",
    "* **Верхняя граница**:\n",
    "\n",
    "  $\n",
    "  Q_3 + 1.5 \\cdot IQR\n",
    "  $\n",
    "\n",
    "Все значения **за пределами этих границ** считаются выбросами.\n",
    "\n",
    "На примере талий девушек журнала Playboy:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e381ec20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_outliers_tukey(data, ax, bins='auto'):\n",
    "    Q1 = data.quantile(0.25)\n",
    "    Q3 = data.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    sns.histplot(data, ax=ax, bins=bins)\n",
    "    ax.axvline(data.mean(), color='r', linestyle='--', label=f'Среднее: {data.mean():.2f}')\n",
    "    ax.axvline(Q1, color='r', linestyle=':', label=f'Q1: {Q1:.2f}')\n",
    "    ax.axvline(Q3, color='r', linestyle=':', label=f'Q3: {Q3:.2f}')\n",
    "\n",
    "    ax.axvspan(data.min(), lower_bound, alpha=0.1, color='red', label=f'Выбросы < {lower_bound:.2f}')\n",
    "    ax.axvspan(upper_bound, data.max(), alpha=0.1, color='red', label=f'Выбросы > {upper_bound:.2f}')\n",
    "    ax.set_xlim((data.min(), data.max()))\n",
    "\n",
    "    # к фильтрации:\n",
    "    mask = (data < lower_bound) | (data > upper_bound)\n",
    "    ax.set_title(f'Выбросы по Tukey: {mask.sum()} из {data.shape[0]} ({mask.sum()/data.shape[0]:.2%})')\n",
    "\n",
    "    ax.legend()\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "plot_outliers_tukey(df_playboy['Waist'], ax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be79676",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим что там у других данных\n",
    "fig, axs = plt.subplots( 1, 3 )\n",
    "fig.set_size_inches( (15, 4) )\n",
    "\n",
    "plot_outliers_tukey(df_reviews['review_count'], ax=axs[0])\n",
    "plot_outliers_tukey(df_telecom['Number vmail messages'], ax=axs[1])\n",
    "plot_outliers_tukey(df_credit['Income'], ax=axs[2])    \n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4ba026",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots( 2, 2 )\n",
    "fig.set_size_inches( (10, 10) )\n",
    "axs = axs.flatten()\n",
    "\n",
    "sns.boxplot(df_reviews['review_count'], ax=axs[0])\n",
    "sns.boxplot(df_telecom['Number vmail messages'], ax=axs[1])\n",
    "sns.boxplot(df_credit['Income'], ax=axs[2])\n",
    "sns.boxplot(df_playboy['Waist'], ax=axs[3])\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492447e3",
   "metadata": {},
   "source": [
    "## 2. Нормализация и шкалирование данных\n",
    "\n",
    "Когда требуется масштабирование данных?\n",
    "1. Для работы с алгоритмами машинного обучения на базе линейной модели (Logistic Regression, SVM, глубокое обучение)\n",
    "2. Для кластерного анализа (k-means, DBSCAN, прочее)\n",
    "3. При работе с алгоритмами поиска ближайших соседей (kNN, ANNOY, прочее)\n",
    "3. Для статистического анализа данных\n",
    "4. При уменьшении размерности данных методами PCA и t-SNE.\n",
    "\n",
    "Существует несколько методов нормализации/шкалирования/масштабирования данных. Вот некоторые из них:\n",
    "\n",
    "1. **Min-Max шкалирование**: Приведение значений признака в диапазон [0, 1].\n",
    "3. **L2-нормализация: или \"горизонтальное шкалирование\"**: Приведение числовых данных к значениям в диапазоне [0, 1], но в рамках каждой записи в таблице.\n",
    "2. **Стандартное шкалирование (Z-оценка, стандартизация)**: Центрирование, приведение данных к нулевому среднему и единичной дисперсии.\n",
    "3. **Робустное шаклирование**: центрирование данных с использованием межквартильного интервала.\n",
    "\n",
    "\n",
    "Эти операции применяются унифицированно ко всем признакам в датасете: \n",
    "- если вы выбрали Min-Max или нормализацию (что-то одно), то все признаки должны быть в диапазоне [0,1], \n",
    "- если выбрали StandardScaler или RobustScaler - то все признаки в итоговом датасете должны быть \"отцентрированы\".\n",
    "\n",
    "Не стоит прибегать к масштабированию данных в следующих случаях:\n",
    "- когда используются деревья решений и другие алгоритмы, основанные на них;\n",
    "- для данных, которые уже примерно в одном масштабе.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e11e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris()\n",
    "df_iris = pd.DataFrame(data=iris.data, columns=iris.feature_names)\n",
    "df_iris['Species'] = iris.target_names[iris.target]\n",
    "sns.pairplot(df_iris, diag_kind='kde', hue='Species', palette='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5911ca",
   "metadata": {},
   "source": [
    "### Min-Max шкалирование\n",
    "\n",
    "$\\tilde{x}_i = \\frac{x_i - x_{min}}{x_{max} - x_{min}}$\n",
    "\n",
    "Применяется, если в датасете:\n",
    "- наличествуют бинарные (0,1) признаки;\n",
    "- данные имеют разные единицы измерения, разный масштаб и требуется привести их к одному диапазону, но при этом с сохранением пропорций \"большой-маленький\";\n",
    "- количественные данные распределены нормально или близко к нормальному;\n",
    "- выбросы в количественных признаках невелики и их масштабирование не приведет к сужению диапазона в рабочих данных.\n",
    "\n",
    "Применяется ко всем признакам в датасете.\n",
    "\n",
    "Категориальные признаки имеет смысл закодировать методом one-hot (и потом, возможно, избавиться от части из них).\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a2b1649",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Min-Max шкалирование\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "df_iris_scaled = df_iris.copy()\n",
    "df_iris_scaled[iris.feature_names] = scaler.fit_transform(df_iris[iris.feature_names])\n",
    "sns.pairplot(df_iris_scaled, diag_kind='kde', hue='Species', palette='Set1')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ee4b88c",
   "metadata": {},
   "source": [
    "### L2-нормализация: или \"горизонтальное шкалирование\"\n",
    "\n",
    "Каждый вектор признаков (строка в таблице) нормируется отдельно, чтобы его длина (евклидова норма) была равна 1.\n",
    "\n",
    "Формула для L2-нормализации:\n",
    "\n",
    "$\\tilde{x}_i = \\frac{x_i}{\\|x\\|_2}$\n",
    "\n",
    "где $\\|x\\|_2 = \\sqrt{\\sum_{j=1}^{n} x_j^2}$ - L2-норма (евклидова длина) вектора признаков.\n",
    "\n",
    "Применяется ко всем признакам в датасете.\n",
    "\n",
    "Данный метод полезен, когда важен не абсолютный масштаб признаков, а их относительные пропорции внутри каждой записи. \n",
    "\n",
    "**ВАЖНО:** L2-номализация уничтожает информацию о масштабе, сохраняя информацию о направлении в многомерном пространстве признаков.  Подходит для задач, где  в качестве метрики используется косинусное расстояние (например, анализ текстов). **Во всех остальных задачах, как правило, L2-нормализация неприменима.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "158c92b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Горизонтальное шкалирование\n",
    "from sklearn.preprocessing import normalize\n",
    "df_iris_horizontal = df_iris.copy()\n",
    "df_iris_horizontal[iris.feature_names] = normalize(df_iris[iris.feature_names], axis=1)\n",
    "\n",
    "x, y = ['petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "fig, axs = plt.subplots( 1, 2 )\n",
    "fig.set_size_inches( (12, 6) )\n",
    "axs[0].set_title('Min-Max Scaler')\n",
    "sns.scatterplot(data=df_iris_scaled, x=x, y=y, hue='Species', ax=axs[0], palette='Set1')\n",
    "axs[1].set_title('L2-нормализация')\n",
    "sns.scatterplot(data=df_iris_horizontal, x=x, y=y, hue='Species', ax=axs[1], palette='Set1')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim((-0.1, 1.1))\n",
    "    ax.set_ylim((-0.1, 1.1))\n",
    "    ax.grid()\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4a54b6d",
   "metadata": {},
   "source": [
    "### Стандартное шкалирование (Z-оценка, стандартизация)\n",
    "\n",
    "$\\tilde{x}_i = \\frac{x_i - \\mu}{\\sigma}$\n",
    "\n",
    "Происходит \"центрирование\" данных относительно начала координат: мат. ожидание приводится к 0, а дисперсия - к 1. Стандартное шкалирование помогает \"сгладить\" выбросы, но если они лежат в широком диапазоне - то данный метод не улучшит ситуацию.\n",
    "\n",
    "Применять в следующих случаях:\n",
    "- **всегда** при работе с Logistic Regression и L2/Ridge регуляризацией;\n",
    "- **всегда** для оценки данных с помощью критерия Колмогорова-Смирнова на соответствие нормальному распределению;\n",
    "- в остальном применим в тех же случаях, что и Min-Max;\n",
    "- данный метод применим, когда в данных присутствуют незначительные выбросы.\n",
    "\n",
    "#### Робустное шкалирование\n",
    "\n",
    "$\n",
    "x_{\\text{scaled}} = \\frac{x - \\text{median}}{\\text{IQR}}\n",
    "= \\frac{x - Q_2}{Q_3 - Q_1}\n",
    "$\n",
    "\n",
    "Где:\n",
    "\n",
    "* $Q_1$ — 25-й перцентиль\n",
    "* $Q_2$ — медиана\n",
    "* $Q_3$ — 75-й перцентиль\n",
    "* $IQR = Q_3 - Q_1$\n",
    "\n",
    "Тоже метод \"центрирования\" данных относительно начала координат. Может использоваться в сочетании со StandardScaler. Применим, когда в данных есть относительно большие выбросы.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b206c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Стандартное шкалирование (Z-оценка, стандартизация)\n",
    "from sklearn.preprocessing import StandardScaler, RobustScaler\n",
    "\n",
    "df_iris_standardized = df_iris.copy()\n",
    "df_iris_standardized[iris.feature_names] = StandardScaler().fit_transform(df_iris[iris.feature_names])\n",
    "df_iris_robust = df_iris.copy()\n",
    "df_iris_robust[iris.feature_names] = RobustScaler().fit_transform(df_iris[iris.feature_names])\n",
    "\n",
    "x, y = ['petal length (cm)', 'petal width (cm)']\n",
    "\n",
    "fig, axs = plt.subplots( 1, 3 )\n",
    "fig.set_size_inches( (12, 4) )\n",
    "axs[0].set_title('Min-Max Scaler')\n",
    "sns.scatterplot(data=df_iris_scaled, x=x, y=y, hue='Species', ax=axs[0], palette='Set1')\n",
    "axs[1].set_title('Standard Scaler')\n",
    "sns.scatterplot(data=df_iris_standardized, x=x, y=y, hue='Species', ax=axs[1], palette='Set1')\n",
    "axs[2].set_title('Robust Scaler')\n",
    "sns.scatterplot(data=df_iris_robust, x=x, y=y, hue='Species', ax=axs[2], palette='Set1')\n",
    "\n",
    "for ax in axs:\n",
    "    ax.set_xlim((-2.1, 2.1))\n",
    "    ax.set_ylim((-2.1, 2.1))\n",
    "    ax.grid()\n",
    "    \n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298b244",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Как это работает на реальных данных?\n",
    "df_credit['Income_std'] = StandardScaler().fit_transform(df_credit[['Income']])\n",
    "df_credit['Income_robust'] = RobustScaler().fit_transform(df_credit[['Income']])\n",
    "\n",
    "df_credit[['Income', 'Income_std', 'Income_robust']].describe().round(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9787bc3c",
   "metadata": {},
   "source": [
    "#### ⁉️ Задание\n",
    "\n",
    "Шкалируйте различные признаки из датасетов, рассмотренных в этом ноутбуке, используя различные методы шкалирования.\n",
    "Сравните результаты и сделайте выводы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2745a752",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ваш код здесь\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2fc8cce",
   "metadata": {},
   "source": [
    "### 3. Визуальный анализ \"сложных\" данных\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "394248d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# влияние масштаба шкал по x и по y, а также количества корзин (bins) в гистограммах\n",
    "sns.set_style('whitegrid')\n",
    "fig, axs = plt.subplots(4, 1)\n",
    "\n",
    "bins = 100\n",
    "\n",
    "data = df_reviews['review_count']\n",
    "# data = df_telecom['Number vmail messages']\n",
    "# data = df_credit['Income']\n",
    "\n",
    "fig.set_size_inches( (8, 12) )\n",
    "axs[0].set_title('Оригинальное распределение')\n",
    "axs[1].set_title('Оригинальное распределение в логарифмическом масштабе по y')\n",
    "axs[2].set_title('Оригинальное распределение в логарифмическом масштабе по x и по y')\n",
    "axs[3].set_title('Шкалированное распределение')\n",
    "\n",
    "data_std = pd.Series(StandardScaler().fit_transform(data.values.reshape(-1, 1)).ravel(), index=data.index)\n",
    "\n",
    "data.hist(ax=axs[0], bins=bins)\n",
    "data.hist(ax=axs[1], bins=bins)\n",
    "data.hist(ax=axs[2], bins=bins)\n",
    "data_std.hist(ax=axs[3], bins=bins)\n",
    "\n",
    "for i in [1,2,3]:  axs[i].set_yscale('log')\n",
    "axs[2].set_xscale('log')\n",
    "\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad284d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Посмотрим как повлияло шкалирование на выбросы по правилу Тьюки\n",
    "fig, axs = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "data = df_reviews['review_count']\n",
    "# data = df_telecom['Number vmail messages']\n",
    "# data = df_credit['Income']\n",
    "\n",
    "data_std = pd.Series(StandardScaler().fit_transform(data.values.reshape(-1, 1)).ravel(), index=data.index)\n",
    "data_robust = pd.Series(RobustScaler().fit_transform(data.values.reshape(-1, 1)).ravel(), index=data.index)\n",
    "\n",
    "\n",
    "\n",
    "plot_outliers_tukey(data, axs[0])\n",
    "plot_outliers_tukey(data_std, axs[1])\n",
    "plot_outliers_tukey(data_robust, axs[2])\n",
    "\n",
    "for i in [0, 1, 2]:  axs[i].set_yscale('log')\n",
    "\n",
    "for ax in axs[-2:]:\n",
    "    ax.set_xlim((-3, 10))\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d77af3e4",
   "metadata": {},
   "source": [
    "## 3. Степенные преобразования (логарифмирование, Box-Cox, Yeo-Johnson)\n",
    "\n",
    "Логарифмирование можно применять только к положительным значениям признака.\n",
    "\n",
    "Если после логарифмирования распределение не сильно приблизилось к нормальныму, можно попробовать Box-Cox или Yeo-Johnson.\n",
    "\n",
    "Yeo-Johnson можно применять к отрицательным и нулевым значениям."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55de16db",
   "metadata": {},
   "source": [
    "### Логарифмирование\n",
    "\n",
    "Логарифмирование имеет смысл применять только если гистограмма напоминает логнормальное распределение: основная мода смещена сильно влево.\n",
    "\n",
    "Логарифмирование не может быть применено к нулевым и отрицательным значениям.\n",
    "\n",
    "Если встречаются нулевые значения - имеет смысл использовать формулу ```log(x + 1)```. Для работы с отрицательными значениями следует использовать преобразование Yeo-Johnson'а.\n",
    "\n",
    "Какое основание выбрать?\n",
    "- если нужна интерпретируемость данных (которые представлены десятичными числами) - ```log10()```;\n",
    "- если нужна интерпретируемость двоичных, битовых данных - ```log2()```;\n",
    "- если не нужна интерпретируемость - ```ln()```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c995596e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Логарифмирование\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, axs = plt.subplots(3, 1)\n",
    "\n",
    "data = df_credit['Income']\n",
    "# data = df_reviews['review_count']\n",
    "\n",
    "fig.set_size_inches( (8, 12) )\n",
    "axs[0].set_title('Оригинальное распределение')\n",
    "axs[1].set_title('Логарифмированные данные')\n",
    "axs[2].set_title('Логарифмированные и затем шкалированные данные')\n",
    "\n",
    "data_log = pd.Series(np.log10(data + 1), index=data.index, name=\"log10()\")\n",
    "data_log_std = pd.Series(StandardScaler().fit_transform(data_log.values.reshape(-1, 1)).ravel(), index=data_log.index, name=\"StandardScaler(log10())\")\n",
    "\n",
    "\n",
    "sns.histplot(data, ax=axs[0], bins=100)\n",
    "sns.histplot(data_log, ax=axs[1], bins=100)\n",
    "plot_outliers_tukey(data_log_std, ax=axs[2], bins=100)\n",
    "\n",
    "for i in range(3): axs[i].set_yscale('log')\n",
    "fig.tight_layout() \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a649433c",
   "metadata": {},
   "source": [
    "### Степенные преобразования Бокса-Кокса и Йео-Джонсона\n",
    "\n",
    "Формула преобразования Бокса-Кокса:\n",
    "\n",
    "$y^{(\\lambda )}_i =\\begin{cases}\\frac{y^\\lambda_i-1}{\\lambda}&\\lambda \\neq 0\\cr \\ln(y) &\\lambda =0\\end{cases}.$\n",
    "\n",
    "и Йео-Джонсона:\n",
    "\n",
    "$y^{(\\lambda )}_i =\\begin{cases}[{(y_i + 1)^\\lambda-1}]/{\\lambda} &\\lambda \\neq 0, y_i \\geq 0 \\cr\\\n",
    "\\ln(y)&\\lambda = 0, y_i \\geq 0 \\cr\\\n",
    "-[(-y_i + 1)^{(2-\\lambda)} - 1]/(2 - \\lambda) &\\lambda \\neq 2, y_i < 0 \\cr\\\n",
    "-\\ln(-y_i+1）&\\lambda = 2, y_i < 0\n",
    "\\end{cases}.$\n",
    "\n",
    "Параметр $\\lambda$ подбирается алгоримами автоматически для лучшей нормализации."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da5c139b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt_boxcox = PowerTransformer(method='box-cox')\n",
    "pt_yeo = PowerTransformer(method='yeo-johnson')\n",
    "\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "fig, axs = plt.subplots(4, 1)\n",
    "\n",
    "data = df_credit['Income']\n",
    "# data = df_reviews['review_count']\n",
    "\n",
    "fig.set_size_inches( (8, 12) )\n",
    "axs[0].set_title('Оригинальное распределение')\n",
    "axs[1].set_title('Логарифмирование + Стандарт')\n",
    "axs[2].set_title('Преобразование Бокса-Кокса')\n",
    "axs[3].set_title('Преобразование Йео-Джонсона')\n",
    "\n",
    "data_log = pd.Series(np.log10(data + 1), index=data.index, name=\"log10(review_count)\")\n",
    "data_log_std = pd.Series(StandardScaler().fit_transform(data_log.values.reshape(-1, 1)).ravel(), index=data_log.index, name=\"StandardScaler(log10())\")\n",
    "\n",
    "data_boxcox = pd.Series(pt_boxcox.fit_transform(data.values.reshape(-1, 1) + 1).ravel(), index=data.index, name=\"Box-Cox\")\n",
    "data_yeo = pd.Series(pt_yeo.fit_transform(data.values.reshape(-1, 1)).ravel(), index=data.index, name=\"Yeo-Johnson\")\n",
    "\n",
    "\n",
    "sns.histplot(data, ax=axs[0], bins=100)\n",
    "sns.histplot(data_log_std, ax=axs[1], bins=100)\n",
    "sns.histplot(data_boxcox, ax=axs[2], bins=100)\n",
    "sns.histplot(data_yeo, ax=axs[3], bins=100)\n",
    "\n",
    "for ax in axs: ax.set_yscale('log')\n",
    "# for ax in axs[1:]: ax.set_xlim((-5, 5))\n",
    "fig.tight_layout() \n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4122acb1",
   "metadata": {},
   "source": [
    "## 4. Бининг🗑️ и квантилизация\n",
    "\n",
    "Биннинг (binning) - это процесс формирования категориальных данных из непрерывных/дискретных количественных данных. Позволяет конвертировать выбросы в категории и сформировать равномерно или событийно наполненные категориальные данные.\n",
    "\n",
    "Квантилизация - это биннинг по квантильным интервалам. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6d8287",
   "metadata": {},
   "source": [
    "Бининг по заданным значениям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5cad4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_credit = pd.read_csv('data/credit_scoring.csv', index_col=0)\n",
    "display(df_credit) \n",
    "df_credit.describe().round(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c8e9e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# разберемся с признаком Income\n",
    "print(f\"Пропусков в Income: {df_credit['Income'].isna().sum()} из {df_credit.shape[0]} ({df_credit['Income'].isna().sum()/df_credit.shape[0]:.2%})\")\n",
    "# создадим series без пропусков\n",
    "s_income = df_credit['Income'].dropna()\n",
    "# и series где пропуски заполнены медианой\n",
    "s_income_median = df_credit['Income'].fillna(df_credit['Income'].median())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d92574",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = [0, 4000, 5000, 6000, 7000, 8000, 2e6]\n",
    "\n",
    "labels = [f\"{border}-{bins[i+1]}\" for i, border in enumerate(bins[:-1]) ]\n",
    "\n",
    "# формируем категории через pd.cut\n",
    "s_income_catg = pd.cut(s_income, bins=bins, labels=labels)\n",
    "display(s_income_catg)\n",
    "\n",
    "# формируем onehot-кодирование категорий\n",
    "df_income_onehot = pd.get_dummies( s_income_catg ).astype(int)\n",
    "df_income_onehot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49708e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(df_income_onehot.sum());"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d85685cb",
   "metadata": {},
   "source": [
    "Квантилизация по децилям:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604de6ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "deciles = s_income.quantile(np.arange(0.1, 1.0, 0.1))\n",
    "deciles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35009e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "s_income.hist(ax=ax, bins=100, alpha=0.7)\n",
    "for pos in deciles:\n",
    "    handle = plt.axvline(pos, color='r')\n",
    "ax.set_yscale('log')\n",
    "ax.set_xscale('log')\n",
    "ax.set_xlabel('Income')\n",
    "ax.set_ylabel('Occurrence')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e935779",
   "metadata": {},
   "outputs": [],
   "source": [
    "deciles_ = pd.concat([pd.Series([0.0]), deciles, pd.Series([2.0e6])])\n",
    "display(deciles_)\n",
    "\n",
    "labels = [f\"{i+1}: {v:.0f}-{deciles_.iloc[i+1]:.0f}\" for i, (q, v) in enumerate(deciles_.iloc[:-1].items()) ]\n",
    "\n",
    "df_income_dec = pd.get_dummies( \n",
    "    pd.cut(s_income, bins=deciles_, labels=labels), \n",
    "    prefix='q', \n",
    "    dtype=np.int64\n",
    ")\n",
    "df_income_dec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307a4e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15, 4))\n",
    "display(df_income_dec.sum())\n",
    "sns.barplot(df_income_dec.sum());"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a40a34c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# можно использовать и pd.qcut()\n",
    "s_income_qcut = pd.qcut(s_income, \n",
    "        10, # кол-во корзин/квантилей\n",
    "        labels=False)\n",
    "s_income_qcut"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c6b4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s_income_qcut.value_counts().sort_index())\n",
    "s_income_qcut.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97929071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# выполним такие же преобразования, но уже на series с пропусками, заполненными медианой\n",
    "s_income_median_qcut = pd.qcut(s_income_median, \n",
    "        10, # кол-во корзин/квантилей\n",
    "        labels=False)\n",
    "print(s_income_median_qcut.value_counts().sort_index())\n",
    "s_income_median_qcut.hist();\n",
    "# как думаете, почему?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b7a14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.histplot(pd.qcut(s_income_median, \n",
    "                         100, \n",
    "                         duplicates=\"drop\", # при наличии одинаковых значений в границах корзин такие корзины будут объединены\n",
    "                         labels=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77bb04e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# посмотрим как справится наш герой на датасете с отзывами на бизнесы из Yelp\n",
    "# данные дискретные, целочисленные, с большим количеством повторяющихся значений\n",
    "df_reviews = pd.read_csv('data/reviews.csv', index_col=0)\n",
    "display(df_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5865c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "s_review_qcut = pd.qcut(df_reviews['review_count'], 10, labels=False)\n",
    "s_review_qcut.hist();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce5a3f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "deciles = df_reviews['review_count'].quantile(np.arange(0.1, 1.0, 0.1))\n",
    "deciles"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e2144c",
   "metadata": {},
   "source": [
    "Другой способ квантильных преобразований:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "433a6d4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "qt = QuantileTransformer(output_distribution='uniform', # можно поставить 'normal'\n",
    "                         random_state=0)\n",
    "\n",
    "_, ax = plt.subplots()\n",
    "ax.set_yscale('log')\n",
    "_ = sns.histplot( qt.fit_transform(df_reviews['review_count'].values.reshape(-1,1)).ravel(), ax=ax, legend=None )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bcac564",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.histplot( QuantileTransformer(output_distribution='uniform').fit_transform(s_income.values.reshape(-1,1)).ravel(), legend=None );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed294ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = sns.histplot( QuantileTransformer(output_distribution='normal').fit_transform(s_income.values.reshape(-1,1)).ravel(), legend=None, kde=True );"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6b9c7d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "260173ef",
   "metadata": {},
   "source": [
    "## Используемые материалы и полезные ссылки\n",
    "\n",
    "- [Using Sklearn’s PowerTransformer (статья на Medium.com)](https://medium.com/@patricklcavins/using-scipys-powertransformer-3e2b792fd712)\n",
    "- [Предварительная обработка данных (статья на scikit-lean.ru, машинный перевод)](https://scikit-learn.ru/6-3-preprocessing-data/)\n",
    "- [Проверка статистических гипотез\n",
    "Дружков П.Н., Золотых Н.Ю., Половинкин А.Н., Чернышова С.Н. статья](http://www.uic.unn.ru/~zny/ml/Old/R/lab2.pdf)\n",
    "- [Мультимодальные распределения: How to Split Multimodal Distributed Data with Gaussian Mixture Models in Python (статья на medium.com)](https://medium.com/@adev94/how-to-split-multimodal-distribution-with-gaussian-mixture-models-in-python-c87957553e4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "courses",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
